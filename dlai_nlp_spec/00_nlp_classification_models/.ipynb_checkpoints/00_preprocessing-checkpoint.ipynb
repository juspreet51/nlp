{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b1fced-9799-42b1-8133-0325b0d16f2a",
   "metadata": {},
   "source": [
    "For dealing with textual data preprocessing, we would be making use of [NLTK](https://www.nltk.org/) library, a leading platform for building Python programs to work with human language data. <br>\n",
    "One can even refer to the [NLTK Book](https://www.nltk.org/book/) available in thier website, for great knowledge on what all can be done using the library, and about NLP in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad721d0-02ae-4f54-bbab-4ef3cca93e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c8c35-68af-4582-ba73-3d3f954840da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4c4407f-72ad-4475-aa7b-a8e1fc14ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51155282-200a-4565-ba47-5c750fb4c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0a1c986-ae8a-4249-a378-8dddce708410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re                                  # library for regular expression operations\n",
    "import string                              # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer        # module for stemming\n",
    "from nltk.tokenize import TweetTokenizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a87dd-6e79-4703-8047-989917f92269",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44258fb8-bc2e-4082-a051-fd107748674d",
   "metadata": {},
   "source": [
    "Lets look at what all awesome things you can do with NLTK library, and why it is so widely used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4a5149-6fb9-4fef-8f08-0cc2407e2ad6",
   "metadata": {},
   "source": [
    "Here, we are going to pull & use one of my favorite book as an input dataset, Meditaions by Marcus Aurelius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a80aec93-a282-47fe-bf80-c0f64e40814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url = \"https://www.gutenberg.org/files/2680/2680-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e249bd72-c63d-4e58-9909-78ca21e93680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of our file: <class 'str'>\n",
      "Number of characters in our file: 424830\n",
      "\n",
      "A sample look:\n",
      " gods granted their favour, irrespective of right or wrong. In this\n",
      "case all devout souls were thrown back upon philosophy, as they had\n",
      "been, though to a less extent, in Greece. There were under the early\n",
      "empire two rival schools which practically divided the field between\n",
      "them, Stoicism and Epicureanism. The ideal set before each was\n",
      "nominally much the same. The Stoics aspired to ἁπάθεια, the repression\n",
      "of all emotion, and the Epicureans to ἀταραξία, freedom from all\n",
      "disturbance; yet in the upshot the one has become a synonym of stubborn\n",
      "endurance, the other for unbridled licence. With Epicureanism we have\n",
      "nothing to do now; but it \n"
     ]
    }
   ],
   "source": [
    "print(f\"Datatype of our file: {type(raw)}\")\n",
    "print(f\"Number of characters in our file: {len(raw)}\")\n",
    "print(f\"\\nA sample look:\\n{raw[11000:11650]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6897bf0-2f41-4dd0-bd3a-e783276d74c0",
   "metadata": {},
   "source": [
    "As we can see, our file, Raw, contains 424830 characters, in String datatype. One of the 1st step we would like to perform in most of our textual preprocessing is, converting the raw string into token, i.e. apply Tokenization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b466fe21-d738-4028-894b-f076cf4ae91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters in raw string: 424830\n",
      "Number of tokens available: 87758\n",
      "Sample tokens:\n",
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Meditations', ',', 'by', 'Marcus', 'Aurelius']\n"
     ]
    }
   ],
   "source": [
    "# Get tokens out of the raw string\n",
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "print(f\"Number of characters in raw string: {len(raw)}\")\n",
    "print(f\"Number of tokens available: {len(tokens)}\")\n",
    "print(f\"Sample tokens:\\n{tokens[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57dd765-6951-46f0-8645-909f108e269c",
   "metadata": {},
   "source": [
    "### Power of NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96224684-4f9c-470d-9aee-bef761d2bf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.text.Text'>\n",
      "Occurence of word Wisdom in the corpus:\n",
      "\n",
      "Displaying 5 of 5 matches:\n",
      "less to his own ability than to his wisdom in choice of lieutenants , shown co\n",
      "who had not attained to the perfect wisdom , certain actions were proper . ( κ\n",
      "ut successful . With a statesman 's wisdom he foresaw the danger to Rome of th\n",
      " also death , a secret of nature 's wisdom : a mixture of elements , resolved \n",
      " as one , whose only study and only wisdom is , to be just in all his actions \n"
     ]
    }
   ],
   "source": [
    "nltk_text = nltk.Text(tokens)\n",
    "print(type(nltk_text))\n",
    "print(f\"Occurence of word Wisdom in the corpus:\\n\")\n",
    "nltk_text.concordance(\"wisdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf691c5-e789-493e-90f0-e93f6f64a687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurence of word Duty in the corpus:\n",
      "Displaying 15 of 15 matches:\n",
      "redecessors , seeking only to do his duty as well as he could , and to keep ou\n",
      "cities done in his name : it was his duty to know , and if he did not he would\n",
      "to confess that he had failed in his duty . But from his own tone in speaking \n",
      "nity of the universe , and on man 's duty as part of a great whole . Public sp\n",
      "s may help him to bear the burden of duty and the countless annoyances of a bu\n",
      "e ' ; but it is not the busy life of duty he has in mind so much as the contem\n",
      "ss , the Roman thought mainly of the duty to be done as well as might be , and\n",
      "of strenuous weariness ; he does his duty as a good soldier , waiting for the \n",
      "which nothing but the stern sense of duty could carry him through . And he did\n",
      "iscommended or commended thou do thy duty : or whether dying or doing somewhat\n",
      " then likewise remember , that every duty that belongs unto a man doth consist\n",
      "ot unto me particularly as a private duty , I will either give it over , and l\n",
      "se , there is not anything . But thy duty towards the Gods also , it is time t\n",
      "apprehension of both true profit and duty , thou canst conceive tolerable ; th\n",
      "ste ; and if Fronto had not done his duty by the young prince , it is not easy\n"
     ]
    }
   ],
   "source": [
    "print(f\"Occurence of word Duty in the corpus:\")\n",
    "nltk_text.concordance(\"duty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddbcbc13-5a64-4599-9888-287d8b12d8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nltk_text.dispersion_plot([\"duty\", \"nature\", \"freedom\", \"morals\", \"spirit\"])\n",
    "# Here, each stripe represents an instance of a word, and each row represents the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea9d8423-71d3-462d-a1e6-189e48271fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of tokens: 87758\n",
      "Count of words in nltk-text: 87758\n",
      "Count of unique words: 7310\n"
     ]
    }
   ],
   "source": [
    "print(f\"Count of tokens: {len(tokens)}\")\n",
    "print(f\"Count of words in nltk-text: {len(nltk_text)}\")\n",
    "print(f\"Count of unique words: {len(set(nltk_text))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35b92bab-597e-45d6-8a66-f843432b664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to', 'be', 'offended', 'with', 'idiots', ',', 'nor', 'unseasonably', 'to', 'set', 'upon', 'those', 'that', 'are', 'carried', 'with', 'the', 'vulgar', 'opinions', ',', 'with', 'the', 'theorems', ',', 'and', 'tenets', 'of', 'philosophers', ':', 'his', 'conversation', 'being', 'an', 'example', 'how', 'a', 'man', 'might', 'accommodate', 'himself', 'to', 'all', 'men', 'and', 'companies', ';', 'so', 'that', 'though', 'his', 'company', 'were', 'sweeter', 'and', 'more', 'pleasing', 'than', 'any', 'flatterer', \"'s\", 'cogging', 'and', 'fawning', ';', 'yet', 'was', 'it', 'at', 'the', 'same', 'time', 'most', 'respected', 'and', 'reverenced', ':', 'who', 'also', 'had', 'a', 'proper', 'happiness', 'and', 'faculty', ',', 'rationally', 'and', 'methodically', 'to', 'find', 'out', ',', 'and', 'set', 'in', 'order', 'all', 'necessary', 'determinations', 'and', 'instructions', 'for', 'a', 'man', \"'s\", 'life', '.', 'A', 'man', 'without', 'ever', 'the', 'least', 'appearance', 'of', 'anger', ',', 'or', 'any', 'other', 'passion', ';', 'able', 'at', 'the', 'same', 'time', 'most', 'exactly', 'to', 'observe', 'the', 'Stoic', '_Apathia_', ',', 'or', 'unpassionateness', ',', 'and', 'yet']\n"
     ]
    }
   ],
   "source": [
    "# Slicing through the indexes like michael scott doing ice-skating\n",
    "print(nltk_text[7310:7450])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdeb706-dd38-4071-a4b0-48f7d2daafae",
   "metadata": {},
   "source": [
    "To be noticed, the file we are using, contains a lot of irrelevant data, which adds no value to our goal, and thus we need to remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92d2fbfe-427a-4afa-bcbe-313b35f63b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of raw before slicing: 424830\n",
      "Length of raw after slicing: 373500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of raw before slicing: {len(raw)}\")\n",
    "begin = raw.find(\"THE FIRST BOOK\")\n",
    "end = raw.rfind(\"*** END OF THE PROJECT GUTENBERG EBOOK MEDITATIONS ***\")\n",
    "raw = raw[begin:end]\n",
    "print(f\"Length of raw after slicing: {len(raw)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12dd4e-793f-401f-91bc-5c1e32b46985",
   "metadata": {},
   "source": [
    "That's enough of playing with NLTK, now lets get back to work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9bbb06-9481-4754-8c6b-f2cc766512bd",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212c736-191a-46e2-9e3f-6492845ef972",
   "metadata": {},
   "source": [
    "## Which Corpus to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e1ed3f-6671-4aa0-9d2a-8d1f8fa304a0",
   "metadata": {},
   "source": [
    "One widely used application of NLP is to prepare model of performing sentiment analysis on human readable data <br>\n",
    "I.e., companies are often interested to know customer's sentiments via thier reviews and feedback, regarding their apps or products <br>\n",
    "And in order to prepare a robust model who can help the companies to tell the customer's or user's sentiment, we need to feed in sentiment reach data <br>\n",
    "Here, if we need to prepare a NLP model which works distinguishly extra-ordinary on classifying human sentiments/emotions, i.e. differentiating which text-input conveys which emotion/sentiment, we need to use data which is filled with all the kinds of emotions. And in such a situation, what other place can we think of, if not Twitter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e0b4ea-fe8c-48dd-8a41-ea1722f223c5",
   "metadata": {},
   "source": [
    "In twitter, we can find sentiments of joy:\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">I am feeling a bit more positive than I was 2 weeks, counselling and talking and getting advice to solve problem really helps. How is everyone doing</p>&mdash; #Shove racism out of society (@AVFCsangha123) <a href=\"https://twitter.com/AVFCsangha123/status/1448956202264711168?ref_src=twsrc%5Etfw\">October 15, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb51d0-35a2-4618-bc12-8f3820484965",
   "metadata": {},
   "source": [
    "We can find grief:\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Just feeling sad because tomorrow is Sunday and then it will be Monday again :(((((</p>&mdash; Giosa Fernandes (@giosafernandes) <a href=\"https://twitter.com/giosafernandes/status/1370879414989422599?ref_src=twsrc%5Etfw\">March 13, 2021</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c144f7-c5c8-416b-9fbb-1e9e5c382008",
   "metadata": {},
   "source": [
    "We can find anger:\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Today I am full of rage.</p>&mdash; Botlett G-S Jr. (@BotlettGSJr) <a href=\"https://twitter.com/BotlettGSJr/status/672204242703269888?ref_src=twsrc%5Etfw\">December 3, 2015</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b076f0-7a73-49c4-9eaa-120732ad2434",
   "metadata": {},
   "source": [
    "And even, boredom:\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">I can&#39;t explain how bored I am right now</p>&mdash; emma (@emmadoherty__) <a href=\"https://twitter.com/emmadoherty__/status/682667697147133952?ref_src=twsrc%5Etfw\">December 31, 2015</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d84e27b-b4ee-4bf8-9c6b-eaf5605f5dde",
   "metadata": {},
   "source": [
    "You got the gist, right? Thus, we would using Twitter corpus available in NLTK library, to train our models, so that the training dataset in sentiment-rich, i.e. abundant of emotions are available. Lets get the data and have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bac1e72a-1f2f-4fb6-8481-36d92ea0df16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                # Python library for NLP\n",
    "from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n",
    "import matplotlib.pyplot as plt            # library for visualization\n",
    "import random                              # pseudo-random number generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1886e49f-c12a-43c5-bc21-a6cefc65c39c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\jaspr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# downloads the twitter corpus\n",
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749d09f8-c476-4cc1-8ece-39e03f91956b",
   "metadata": {},
   "source": [
    "Our corpus comes with labeled data, i.e. Positive and Negative tweets are already distictly available, so next we would separating them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a23d6375-29b0-4cf4-9a9d-3c466c00d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "tweets_positive = twitter_samples.strings('positive_tweets.json')\n",
    "tweets_negative = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06743e46-1663-4465-9134-6830cf865d92",
   "metadata": {},
   "source": [
    "Lets look at how much data do we have at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2038bcdf-9bcd-4cdf-8b56-0273b64af765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n",
      "\n",
      "The data-type of tweets_positive variable is:  <class 'list'>\n",
      "The data-type of tweets_negative variable is:  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive tweets: ', len(tweets_positive))\n",
    "print('Number of negative tweets: ', len(tweets_negative))\n",
    "\n",
    "print('\\nThe data-type of tweets_positive variable is: ', type(tweets_positive))\n",
    "print('The data-type of tweets_negative variable is: ', type(tweets_negative))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed3dae3-22d7-4bc4-be39-d3c865d2a49a",
   "metadata": {},
   "source": [
    "We have got 1k tweets, as list of strings, which are 5k positive tweets and 5k negative tweets <br>\n",
    "Lets peek at some random sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d09e0d50-31cd-4b63-9b9c-144cd9f35456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random positive tweet: @Mish23615351  follow @jnlazts &amp; http://t.co/RCvcYYO0Iq follow u back :)\n",
      "Random negative tweet: @SophiaMascardo happy trip, keep safe. see you soon :* :(\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random_index = random.randint(0,15) #Return random integer in range [a, b], including both end points\n",
    "print(f\"Random positive tweet: {tweets_positive[random_index]}\")\n",
    "print(f\"Random negative tweet: {tweets_negative[random_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de7571-d814-485b-8e61-15ea9f9d06c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Steps involved in Textual Data Preprocessing :\n",
    "We are ready to apply our preprocessing steps to the data. We would applying following essential data preprocessing:\n",
    "- Removing noise\n",
    "- Tokenizing the string\n",
    "- Lowercasing\n",
    "- Removing stop words and punctuation\n",
    "- Stemming\n",
    "** <i>If wondering what does all these mean, definition and meaning of all these terms have been explained in previous chapter, visit them, if not already gone through</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73c692e-ee7e-466d-9070-dfdb0c049900",
   "metadata": {},
   "source": [
    "### Removing Noise\n",
    "Noise is the part of our data, which adds no value, e.g. For sentiment analysis usecase userids, dates, links, etc. contribute no useful information<br>\n",
    "So we will begin with their removal fist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eefe4f0c-223b-4408-9731-92edef32c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tweet_noise(tweet_list):\n",
    "    tweets_noise_reduction_1 = [re.sub(r'^RT[\\s]+', '', item) for item in tweet_list] # Removing \"RT\" text from the tweets\n",
    "    tweets_noise_reduction_2 = [re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', item) for item in tweets_noise_reduction_1] # Removing hyperlinks\n",
    "    tweets_noise_reduction_3 = [re.sub(r'#', '', item) for item in tweets_noise_reduction_2] # removing hashtag symbol\n",
    "    tweets_noise_reduction_4 = [re.sub(r'@[A-Za-z0-9]+', '',item) for item in tweets_noise_reduction_3] # removing userids\n",
    "    print(f\"Sample tweet post treatment:\\n{tweets_noise_reduction_4[random.randint(1,100)]}\")\n",
    "    return tweets_noise_reduction_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "646f31b3-cbc5-43ed-bce8-d9c9c1416523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tweet post treatment:\n",
      "  follow  &amp; \n",
      "Sample tweet post treatment:\n",
      "So much misses :-( \n"
     ]
    }
   ],
   "source": [
    "tweets_positive = remove_tweet_noise(tweets_positive)\n",
    "tweets_negative = remove_tweet_noise(tweets_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49336fa6-78e8-4c21-ab1b-a244930081d6",
   "metadata": {},
   "source": [
    "### Tokenizing the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0828b378-968f-4163-a29b-32f44208fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e701341-b5e2-4e8f-8b86-dd6ad80a8969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string:\n",
      "[['followfriday', '_inte', '_paris', 'for', 'being', 'top', 'engaged', 'members', 'in', 'my', 'community', 'this', 'week', ':)'], ['hey', 'james', '!', 'how', 'odd', ':/', 'please', 'call', 'our', 'contact', 'centre', 'on', '02392441234', 'and', 'we', 'will', 'be', 'able', 'to', 'assist', 'you', ':)', 'many', 'thanks', '!'], ['we', 'had', 'a', 'listen', 'last', 'night', ':)', 'as', 'you', 'bleed', 'is', 'an', 'amazing', 'track', '.', 'when', 'are', 'you', 'in', 'scotland', '?', '!'], ['congrats', ':)'], ['yeaaah', 'yipppy', '!', '!', '!', 'my', 'accnt', 'verified', 'rqst', 'has', 'succeed', 'got', 'a', 'blue', 'tick', 'mark', 'on', 'my', 'fb', 'profile', ':)', 'in', '15', 'days']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_tweets(tweet_list):\n",
    "    # instantiate tokenizer class\n",
    "    nltk_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tokens = [nltk_tokenizer.tokenize(item) for item in tweets_positive]\n",
    "    return tokens\n",
    "    \n",
    "# tokenize tweets\n",
    "tweets_positive_tokens = tokenize_tweets(tweets_positive)\n",
    "tweets_negative_tokens = tokenize_tweets(tweets_negative)\n",
    "\n",
    "print('Tokenized string:')\n",
    "print(tweets_negative_tokens[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b97ee-d99a-42e9-a573-49347dc34f38",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2745b18-e964-4068-b868-cc0b3c882439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_tweets(tweet_list):\n",
    "    case_converted = [word.lower() for tweet in tweet_list for word in tweet]\n",
    "    print(f\"Sample tweet post treatment:\\n{case_converted[random.randrange(5,15,1)]}\")\n",
    "    return case_converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5a2904e6-7f17-4328-bc02-90c561aef31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tweet post treatment:\n",
      ":)\n",
      "Sample tweet post treatment:\n",
      "my\n"
     ]
    }
   ],
   "source": [
    "# lowercase tweets\n",
    "tweets_positive_case_transformed = lowercase_tweets(tweets_positive_tokens)\n",
    "tweets_negative_case_transformed = lowercase_tweets(tweets_negative_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a8df3-7632-4fa2-bd28-9ec30f802fee",
   "metadata": {},
   "source": [
    "### Removing stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3fbf80f4-cc24-4c70-bbad-ce6c60755763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of Stop Words: ['i', 'me', 'my', 'myself', 'we']\n",
      "Example of Punctuation: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_english = stopwords.words('english') \n",
    "print(f\"Example of Stop Words: {stopwords_english[:5]}\")\n",
    "print(f\"Example of Punctuation: {string.punctuation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d6d55ac-eaf7-48be-a8de-5286b6919d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(tweets_list):\n",
    "    clean_tweets = []\n",
    "    \n",
    "    # Now we will pick only those tokens, which don't fall under the category of stopwords & punctuations marks\n",
    "    for word in tweets_list: # Go through every word in your tokens list\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "            word not in string.punctuation): # remove punctuation\n",
    "            clean_tweets.append(word)\n",
    "    return clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbed81f0-01e7-46ab-ada6-138d2fb25d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_positive_cleaned = remove_stop_words(tweets_positive_case_transformed)\n",
    "tweets_negative_cleaned = remove_stop_words(tweets_negative_case_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcddda1-6e2c-4e33-9b5a-b37c00774cf2",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "*** <i>definitions provided in previous chapter</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ba33b2dd-49b0-40e3-a24e-d300cc270880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_stemming(tweets_lists):\n",
    "    # Instantiate stemming class\n",
    "    stemmer = PorterStemmer() \n",
    "    \n",
    "    # Create an empty list to store the stems\n",
    "    tweets_stemed = [] \n",
    "\n",
    "    for word in tweets_lists:\n",
    "        stem_word = stemmer.stem(word)  # stemming word\n",
    "        tweets_stemed.append(stem_word)  # append to the list\n",
    "    return tweets_stemed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff3fb75c-81c1-478e-ad1c-631b161bdcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_positive_stemmed = tweet_stemming(tweets_positive_cleaned)\n",
    "tweets_negative_stemmed = tweet_stemming(tweets_negative_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4605a2f-8806-42a9-95cf-059f683f66b8",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95900c29-e967-4805-bdbf-16da7ac4d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing (validation set) \n",
    "test_pos = tweets_positive_stemmed[4000:]\n",
    "train_pos = tweets_positive_stemmed[:4000]\n",
    "test_neg = tweets_negative_stemmed[4000:]\n",
    "train_neg = tweets_negative_stemmed[:4000]\n",
    "\n",
    "train_x = train_pos + train_neg \n",
    "test_x = test_pos + test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77e07b-4c2d-43b3-815e-839388e96a65",
   "metadata": {},
   "source": [
    "Creating 1-0 label for +ve & -ve class, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "812a99e0-5635-4090-9891-c6ab49f04ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "22e9089d-cc48-4dcc-b889-297c609e04ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y.shape = (8000, 1)\n",
      "test_y.shape = (60916, 1)\n"
     ]
    }
   ],
   "source": [
    "# Print the shape train and test sets\n",
    "print(\"train_y.shape = \" + str(train_y.shape))\n",
    "print(\"test_y.shape = \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02839293-7b4b-4506-81ad-16bac1727f3f",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "* The sigmoid function is defined as: \n",
    "\n",
    "$\\large h(z) = \\frac{1}{1+\\exp^{-z}} $\n",
    "\n",
    "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "51827dd9-7346-404d-8f9f-87aa0dcdadda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "def sigmoid(z):\n",
    "    h = 1/(1+exp(-z))\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cf17d04-4fe6-418e-bd24-bbc4d8338833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "# Testing your function \n",
    "if (sigmoid(0) == 0.5):\n",
    "    print('SUCCESS!')\n",
    "else:\n",
    "    print('Oops!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45851e83-e4a9-4581-a925-c78a90a33196",
   "metadata": {},
   "source": [
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "Regression:\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "Note that the $\\theta$ values are \"weights\". If you took the Deep Learning Specialization, we referred to the weights with the `w` vector.  In this course, we're using a different variable $\\theta$ to refer to the weights.\n",
    "\n",
    "Logistic regression\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
    "We will refer to 'z' as the 'logits'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349b0b0d-a922-45d8-82a0-d2276415e478",
   "metadata": {},
   "source": [
    "### Cost function and Gradient\n",
    "\n",
    "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
    "\n",
    "$$\\large J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) $$\n",
    "* $m$ is the number of training examples\n",
    "* $y^{(i)}$ is the actual label of the i-th training example.\n",
    "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
    "\n",
    "The loss function for a single training example is\n",
    "$$\\large Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n",
    "\n",
    "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
    "* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0. \n",
    "* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0. \n",
    "* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "619ca76a-0ca5-4f03-9e41-b05c941351ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976294"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n",
    "-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1619a4be-f30f-40d7-98cb-6a372de0015b",
   "metadata": {},
   "source": [
    "* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number: $-1 \\times log(0.0001) \\approx 9.2$.  The closer the prediction is to zero, the larger the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "202e6902-fb04-42ec-aa99-944ba5c641f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.210340371976182"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n",
    "-1 * np.log(0.0001) # loss is about 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb16afb5-81b7-4772-8963-37c0ca53e295",
   "metadata": {},
   "source": [
    "#### Update the weights\n",
    "\n",
    "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
    "The gradient of the cost function $J$ with respect to one of the weights $\\large \\theta_j$ is:\n",
    "\n",
    "$$\\large \\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x^{(i)}_j $$\n",
    "* 'i' is the index across all 'm' training examples.\n",
    "* 'j' is the index of the weight $\\large \\theta_j$, so $x^{(i)}_j$ is the feature associated with weight $\\large \\theta_j$\n",
    "\n",
    "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\large \\alpha$:\n",
    "$$\\large \\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "* The learning rate $\\large \\alpha$ is a value that we choose to control how big a single update will be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341fb50-73a7-4125-b93f-450e62413482",
   "metadata": {},
   "source": [
    "## Instructions: Implement gradient descent function\n",
    "* The number of iterations `num_iters` is the number of times that you'll use the entire training set.\n",
    "* For each iteration, you'll calculate the cost function using all training examples (there are `m` training examples), and for all features.\n",
    "* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:  \n",
    "$$\\mathbf{\\theta} = \\begin{pmatrix}\n",
    "\\theta_0\n",
    "\\\\\n",
    "\\theta_1\n",
    "\\\\ \n",
    "\\theta_2 \n",
    "\\\\ \n",
    "\\vdots\n",
    "\\\\ \n",
    "\\theta_n\n",
    "\\end{pmatrix}$$\n",
    "* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (note that the corresponding feature value $\\mathbf{x_0}$ is 1).\n",
    "* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'.  $z = \\mathbf{x}\\mathbf{\\theta}$\n",
    "    * $\\mathbf{x}$ has dimensions (m, n+1) \n",
    "    * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n",
    "    * $\\mathbf{z}$: has dimensions (m, 1)\n",
    "* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n",
    "* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'.  Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.\n",
    "$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n",
    "* The update of theta is also vectorized.  Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:\n",
    "$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
